---
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
header-includes: \usepackage{float}
---

\begin{centering}

\vspace*{5 cm}

\Huge

{\bf Descubrimiento del Conocimiento usando herramientas de Big Data Módulo 2}

\vspace{3 cm}

\Large
Marco Andrés Vázquez Hernández

\vspace{1 cm}
\normalsize
Práctica KNN. 

Septiembre de 2018

\normalsize
Instituto Politécnico Nacional


\end{centering}

\newpage



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Descripción

Con las muesras de entrenamiento, clasificar las nuevas instancias. Suerte

# Carga de archivos


```{r}
setwd("C:/Users/marco/IPN_BigData/Modulo2/Práctica_KNN")

train<-read.csv("Ejercicio Knn.csv", skip=1, nrows= 12)
eval<-read.csv("Ejercicio Knn.csv", skip=16, nrows=4, header=F)
colnames(eval)<-colnames(train)

```

# Transformación de datos
Se convirtió la variable "Invertir" a dicotómica y se juntaron los datos para tomar todos los valores en la normalización.

```{r}
train$Invertir<-ifelse(as.character(train$Invertir)=="Si",1,0)
eval$Invertir<-ifelse(as.character(eval$Invertir)=="Si",1,0)
dats<-rbind(train,eval)
```

Se creó la función para normalizar los datos:

```{r}
Normalizar<- function (x){
  if (all(is.na(x))){
    return(rep(NA, length(x)))
  } else if (sum(x)==0){
    return(rep(0,length(x)))
  } else if (min(x)==max(x) & max(x)!=0){
    return(rep(1,length(x)))
  } else
  return((x-min(x))/(max(x)-min(x)))
} 
```

Se aplicó a la base con todos los valores y después se separaron de nuevo el conjunto de entrenamiento y el de evaluación:

```{r}
aux<-sapply(dats[,1:(ncol(dats)-1)], Normalizar)
train2<-as.data.frame(cbind(aux[1:nrow(train),], train[,ncol(train)]))
colnames(train2)[ncol(train2)]<-"Invertir"
eval2<-as.data.frame(cbind(aux[(nrow(train)+1):nrow(aux),], eval[,ncol(eval)]))
colnames(eval2)[ncol(eval2)]<-"Invertir"
```

Una muestra de los datos normalizados queda:
```{r}
head(train2)
head(eval2)
```

# Algoritmo

Se crearon matrices para medir las distancias de los puntos de evaluación a cada uno de los puntos en el conjunto de entrenamiento:

```{r}

aux<-data.frame()
for(i in 1:nrow(eval2)){
  for(j in 1:nrow(train2)){
    aux[j,i]<-dist(rbind(train2[j,-ncol(train2)], eval2[i,-ncol(eval2)]), method="euclidean")    
  }
}
euclidian<-aux
euclidian

aux<-data.frame()
for(i in 1:nrow(eval2)){
  for(j in 1:nrow(train2)){
    aux[j,i]<-dist(rbind(train2[j,-ncol(train2)], eval2[i,-ncol(eval2)]), method="maximum") 
  }
}
max<-aux
max
```

Se creó la función para tomar el promedio del pronóstico de los k-vecinos más cercanos de cada punto de evaluación:

```{r}

kesimo<-function(v,k){
  mean(train2$Invertir[which(v %in% sort(v)[1:k])])
}

```

Se aplicó a cada punto de evaluación para k=1,3 y 5 para la distancia euclidiana quedando:

```{r}

matriz_inversion_e<-data.frame()
for(v in 1:nrow(eval2)){
  for(k in 1:3){
    matriz_inversion_e[v,k]<-kesimo(euclidian[,v],k*2-1)
  }
}
colnames(matriz_inversion_e)<-c("k=1","k=3","k=5")
matriz_inversion_e
```

De donde se puede observar que para k=1 sugiere invertir en todos los casos, mientras que para k=3 y 5; se puede definir un parámetro de "confianza", si se tomara .5 (redondeo) se invertiría en todos los casos, si se tomara un parámetro de confianza más alto, tal vez quedarían fuera las inversiones en los casos 3 y 4 (e incluso 2).

Para la distancia máxima se tiene:

```{r}

matriz_inversion_max<-data.frame()
for(v in 1:nrow(eval2)){
  for(k in 1:3){
    matriz_inversion_max[v,k]<-kesimo(max[,v],k*2-1)
  }
}
colnames(matriz_inversion_max)<-c("k=1","k=3","k=5")
matriz_inversion_max
```

En donde para k=1 se sugiere invertir en todos los casos y para k=3 y 5 podría variar de acuerdo a un parámetro de "confianza".
